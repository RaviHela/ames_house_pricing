---
title: "house_price_pre_3"
author: "Ravi Hela"
date: "22/03/2020"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

## Load libraries

```{r}
library(tidymodels)
library(readr)
library(tidyverse)
library(lubridate)
library(skimr)
library(DataExplorer)
library(tidyquant)
library(corrr)
library(caret)
library(vip)
library(Hmisc)


```


## define some useful functions
```{r}
#function to help filter corelated variables
split_arrange_names <- function(x){
  
  return(paste0(sort(unlist(str_split(x, "_"))), collapse = ""))
}

#get simplifeid correlation dataframe
get_corr_df_simple <- function(df, cut_off = NA) {
  
  simpl_df <-
    df %>% select_if(is.numeric) %>% correlate(use = "pairwise.complete.obs") %>%
    pivot_longer(
      -rowname,
      names_to = "features",
      values_to = "corr",
      values_drop_na = T
    ) %>%
    mutate(t1 = paste(rowname, features, sep = "_")) %>%
    mutate(t2 = sapply(t1, split_arrange_names)) %>%
    arrange(t2) %>%
    group_by(t2) %>%
    mutate(rank = row_number(t2)) %>%
    ungroup %>% filter(rank == 1) %>%
    select(rowname, features, corr) %>%
    arrange(desc(abs(corr))) 
  
  if(is.na(cut_off)){
    return(simpl_df)
  }else{
    
    return(simpl_df %>% filter(abs(corr) >= cut_off))
  }
  
}

#get model metrics in a dataframe
get_model_metrics <- function(model, df){
  
  #model_type <- str_replace(class(model)[[1]] , "_", "")
  model_type <- model[[2]]
  
  #print(model)
  # class(model)
  # print(model[[1]])
  # print(model[[2]])
  
  #print(deparse(substitute(model)))

  #print(names({{model}}))
  df %>%
    bind_cols(predict(model[[1]], df)) %>%
    select(log_sale_price, predicted = .pred) %>%

    #add squared error
    mutate(
      sq_error_log = (predicted - log_sale_price) ^ 2,
      sale_price = exp(log_sale_price),
      predicted_sp = exp(predicted),
      sq_error = predicted_sp - sale_price
    ) %>%

    #plot erro
    #ggplot(aes(x = sq_error, y = predicted_lm)) + geom_point() + theme_light()
    mutate(truth = log(sale_price),
           estimate = log(predicted_sp)) %>%


    #get model metrics
    metrics(truth, estimate) %>%
    select(-.estimator) %>%
    mutate(m_type  = model_type) %>%
    filter(.metric == "rmse")
  }

```


## Read data
```{r}

train <- read_csv("train.csv")
test <- read_csv("test.csv")

 
names(train) <- make.names(names(train))
names(test) <- make.names(names(test))

```

## combinie test and train for eda

```{r}
lapply(list(train = train, test = test), skim)
test$SalePrice <- NA
test$set <- "test"
train$set <- "train"
house_comb <- bind_rows(train, test)
skim(house_comb)

```

## Eda

Doing an EDA for data quality checks. We first check for missing values

```{r}
profile_missing(house_comb) %>%
  arrange(desc(pct_missing))

plot_missing(house_comb)


```

The missing values is result of absence of a particular feature in the property. We will code the missing values as with approproate categorical variable later on.

# Further Data Exploration

```{r}
#check distribution of repsonse variabes

house_comb %>% 
  filter(set == "train") %>%
  mutate(log_sale_price = log10(SalePrice)) %>%
  select(SalePrice, log_sale_price) %>%
  pivot_longer(everything(), names_to = "Sale", values_to = "price") %>%
  ggplot(aes(x = price)) +
  geom_histogram() + facet_wrap(Sale~., scales = "free") +
  labs(title = "distribution of Price") +
  theme_bw()

```

Sale Price distribtion is right skewed however log transformation is Normal and is a candidate for Linear Regression.
Lets explore other features.


# Check distribution of categorical variables

```{r}
plot_bar(house_comb, ggtheme = theme_minimal())

```

# Check distribution of Numeric features

```{r}

plot_histogram(house_comb, ggtheme = theme_minimal())

```


# Explore categorical variables against Sale price.
```{r}

house_comb %>% mutate(SalePrice = SalePrice/1000) %>%
plot_boxplot(by = "SalePrice") 

```


# Checking correlation among variables

```{r}

house_comb %>%
  select_if(is.numeric) %>%
  corrr::correlate() %>%
  rearrange() %>%
  shave() %>% rplot + theme(axis.text.x = element_text(angle = 90))

house_comb %>% get_corr_df_simple(cut_off = 0.6)

```


# Check for near zero variables

```{r}

house_comb[, nearZeroVar(house_comb)] %>%
  mutate_if(is.character, as.factor) %>%
  summary
```


# Curious to know if street type makes a difference
```{r}
house_comb %>% filter(set == "train") %>%
  group_by(Street) %>%
  summarise(sumry = list(summary(SalePrice))) %>% 
  mutate(sumry = map(sumry, ~ data.frame(t(.)))) %>%
  unnest(sumry) %>% pivot_wider(names_from = Street, values_from = Freq)

```

In general we do see House in pavement have higher Sale price than the Gravel.



# Checking affect of time over Sale Price

```{r}

house_comb %>% 
  filter(set == "train") %>%
  select_at(vars(starts_with("Year"), starts_with("MoSold"), SalePrice)) %>%
  pivot_longer(-SalePrice, names_to = "time_metric", values_to = "unit") %>%
  ggplot(aes(y = SalePrice, x = unit )) + geom_point() + facet_wrap(.~time_metric, scales = "free")
  


house_comb %>% 
  filter(set == "train") %>%
  select_at(vars(starts_with("Year"), starts_with("MoSold"), SalePrice)) %>%
  pivot_longer(-SalePrice, names_to = "time_metric", values_to = "unit") %>%
  ggplot(aes(y = SalePrice, x = as.factor(unit) )) + geom_boxplot() + facet_wrap(.~time_metric, scales = "free") + theme_light()
  

```


There is no particular effect of month sold on Sale price. However, In general we dprices does increase Year on Year which makes common sense.


# Based on our explorations above lets will do some preliminary data cleaning on the train data set

The code below will fix few missing values which are actually features not present in a property.
These features are related to
. Alley
. Bsmt
. Garage
. FireplaceQu
. PoolQC
. Fence
. MiscFeatures
. MasVnr

We also create a categorical bucketing variable out of YearSold and Yr Modelled. We create new variabe 'log_sale_price' that is log of Sale Price. This new variable will be used as  response variables.

Last we remove Id, Sale Price and set variable that will not contribute to the model.

```{r}

#pp3
house <- train %>%
  
  #converting sales to log scale
  mutate(log_sale_price = log(SalePrice)) %>%
  
  
  #fill in missing values NA for factor variables as per data description
  
  #Alley
  mutate(Alley = if_else(is.na(Alley), "No Alley", Alley)) %>%
  
  #Bsmt
  mutate(
    BsmtCond = if_else(is.na(BsmtCond), "No Bsmnt", BsmtCond),
    BsmtExposure =  if_else(is.na(BsmtExposure), "No Bsmnt", BsmtExposure),
    BsmtQual = if_else(is.na(BsmtQual), "No Bsmnt", BsmtQual),
    BsmtFinType1 =  if_else(is.na(BsmtFinType1), "No Bsmnt", BsmtFinType1),
    BsmtFinType2 =  if_else(is.na(BsmtFinType2), "No Bsmnt", BsmtFinType2),
    BsmtFinSF1 =  if_else(is.na(BsmtFinSF1), 0, BsmtFinSF1),
    BsmtFinSF2 =  if_else(is.na(BsmtFinSF2), 0, BsmtFinSF2),
    BsmtUnfSF =  if_else(is.na(BsmtUnfSF), 0, BsmtUnfSF),
    TotalBsmtSF =  if_else(is.na(TotalBsmtSF), 0, TotalBsmtSF),
    BsmtFullBath =  if_else(is.na(BsmtFullBath), 0, BsmtFullBath),
    BsmtHalfBath =  if_else(is.na(BsmtHalfBath), 0, BsmtHalfBath)
    
  ) %>%
  
  #Garage
  #GarageType, GarageType, GarageFinish, GarageQual, GarageCond, 'GarageYrBlt', 'GarageArea', 'GarageCars'
  mutate(
    GarageType = if_else(is.na(GarageType), "No Garage", GarageType),
    GarageFinish =  if_else(is.na(GarageFinish), "No Garage", GarageFinish),
    GarageQual = if_else(is.na(GarageQual), "No Garage", GarageQual),
    GarageCond =  if_else(is.na(GarageCond), "No Garage", GarageCond),
    GarageYrBlt =  if_else(is.na(GarageYrBlt), 0, GarageYrBlt),
    GarageArea =  if_else(is.na(GarageArea), 0, GarageArea),
    GarageCars =  if_else(is.na(GarageCond), 0, GarageCars)
  ) %>%
  
  mutate(MasVnrType = if_else(is.na(MasVnrType), "No MasVnrType", MasVnrType)) %>%
  mutate(MasVnrArea = if_else(is.na(MasVnrArea), 0, MasVnrArea)) %>%
  
  
  #FireplaceQu, PoolQC,Fence, MiscFeature
  mutate(
    FireplaceQu = if_else(is.na(FireplaceQu), "No Fireplc", FireplaceQu),
    PoolQC =  if_else(is.na(PoolQC), "No Pool", PoolQC),
    Fence = if_else(is.na(Fence), "No Fence", Fence),
    MiscFeature =  if_else(is.na(MiscFeature), "None", MiscFeature)
  ) %>%
  
  mutate(YrBuilt_cat = cut2(YearBuilt, cuts = seq(min(YearBuilt), max(YearBuilt), 5))) %>%
  
  mutate(YearRemodAdd = if_else(is.na(YearRemodAdd), YearBuilt, YearRemodAdd)) %>%
  
  
  #remove non contributing features and IDs
  select(-Id,-SalePrice, -set)
```



# Creating a Training and Validation set using rsample
```{r}

set.seed(1234)
house_split <- initial_split(house)

train_split <- training(house_split)

```


# Create recepies with series of preprocessing step on training set

```{r}

house_recipe <- train_split %>%
  recipe(log_sale_price ~ .) %>%
  step_string2factor(all_nominal(), -all_outcomes()) %>%
  step_num2factor(MoSold, levels = as.character(unique(train_split$MoSold))) %>%
  step_num2factor(YrSold, levels = as.character(unique(train_split$YrSold))) %>%
  step_num2factor(MSSubClass, levels = as.character(unique(train_split$MSSubClass))) %>%
  step_num2factor(OverallCond, levels = as.character(unique(train_split$OverallCond))) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  step_corr(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_knnimpute(all_predictors(), -all_outcomes()) 
  # step_corr(all_predictors()) %>%
  # step_nzv(all_predictors()) %>%
  
doParallel::registerDoParallel()
house_prep <- prep(house_recipe)


house_recipe_treebased <- train_split %>%
  recipe(log_sale_price ~ .) %>%
  step_string2factor(all_nominal(), -all_outcomes()) %>%
  step_num2factor(MoSold, levels = as.character(unique(train_split$MoSold))) %>%
  step_num2factor(YrSold, levels = as.character(unique(train_split$YrSold))) %>%
  step_num2factor(OverallCond, levels = as.character(unique(train_split$OverallCond))) %>%
  step_num2factor(MSSubClass, levels = as.character(unique(train_split$MSSubClass))) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  # step_center(all_numeric(), -all_outcomes()) %>%
  # step_scale(all_numeric(), -all_outcomes()) %>%
  step_corr(all_numeric()) %>%
  step_nzv(all_numeric()) %>%
  step_knnimpute(all_predictors(), -all_outcomes())

doParallel::registerDoParallel()    
house_treebased_prep <- prep(house_recipe_treebased)

```


# Applying preprocessing on training and validation(testing) set 
```{r}
house_train <- juice(house_prep)
house_test <- bake(house_prep, testing(house_split))

#splits for tree based algo
house_train_treebased <- juice(house_treebased_prep)
house_test_treebased <- bake(house_treebased_prep, testing(house_split))
```



## Train models

# linear regression
```{r}

#linear model
lm_model <- #recipe(log_sale_price ~ . , data = house_train) %>%
  linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm") %>%
  fit(log_sale_price ~ . , data = house_train)


```

# random forest
```{r}

#create recepie on the preped house train data
rf_rec <-
  recipe(log_sale_price ~. , data = house_train_treebased)

#give model spec
rf_mod <-
  rand_forest(mtry = tune(), min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression")

#create Search grid
rf_grid <-
  grid_regular(mtry(range = c(15,40)), min_n(range= c(10, 2)), levels = 5)

#create samples for cross validation
folds <- vfold_cv(house_train_treebased, v = 10)


doParallel::registerDoParallel()

#create models with grid search
rf_res <-
  tune_grid(model = rf_mod, rf_rec,resamples = folds ,  grid = rf_grid)

final_mtry <- select_best(rf_res, "rmse", maximize = FALSE)$mtry
final_min_node <- select_best(rf_res, "rmse", maximize = FALSE)$min_n

#random forest
rf_model <- rand_forest(mtry = final_mtry, min_n = final_min_node) %>%
  set_mode("regression") %>%
  set_engine("ranger",importance = 'impurity') %>%
  fit(log_sale_price ~ . , data = house_train_treebased)

#Visualise Feature Importance
vi(rf_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)), abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>%
ggplot(aes(x = Variable, y = abs_importance)) + geom_col() + coord_flip()

```


# xgboost
```{r}

doParallel::registerDoParallel()
#xgboost
xg_model <- boost_tree() %>%
  set_mode("regression") %>%
  set_engine("xgboost") %>%
  fit(log_sale_price ~ . , data = house_train_treebased)

```


# linear model ridge
```{r}

ridge_rec <-
  recipe(log_sale_price ~. , data = house_train)

ridge_mod <-
  linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")
 

ridge_grid <- expand.grid(penalty = seq(from = 0.01, to = 0.2, by=0.001), mixture = 0)

folds <- vfold_cv(house_train, v = 10)

doParallel::registerDoParallel()

ridge_res <-
  tune_grid(ridge_rec, model = ridge_mod, resamples = folds ,  grid = ridge_grid)

autoplot(ridge_res)

final_penalty <- select_best(ridge_res, "rmse", maximize = FALSE)$penalty

#apply best tuning parameter linear model ridge
lm_ridge_model <- linear_reg(penalty = final_penalty , mixture = 0) %>%
  set_mode("regression") %>%
  set_engine("glmnet") %>%
  fit(log_sale_price ~. , data = house_train)

#Visualise Feature Importance
vi(lm_ridge_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)), abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>%
ggplot(aes(x = Variable, y = abs_importance, fill = Sign)) + geom_col() + coord_flip()

vi(lm_ridge_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)), abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>% ggplot(aes(x = Variable, y = Importance, fill = Sign)) + geom_col() + coord_flip()

#Visulaise beta strength
as.data.frame(as.matrix(lm_ridge_model$fit$beta)) %>%  rownames_to_column() %>% pivot_longer(-rowname) %>% group_by(rowname) %>% summarise(mean_beta = mean(value), se_beta = sd(value)) %>% ungroup %>% ggplot(aes(y = 1, x = mean_beta, , size = abs(mean_beta)), alpha = 0.2) + geom_text(aes(label = rowname), position = position_jitter())


```





# linear model lasso
```{r}

lasso_rec <-
  recipe(log_sale_price ~. , data = house_train)

lasso_mod <-
  linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")
 
lasso_grid <- expand.grid(penalty = seq(from = 0.001, to = 0.005, by = 0.000001), mixture = 1)

folds_lasso <- vfold_cv(house_train, v = 10)

lasso_res <-
  tune_grid(lasso_rec, model = lasso_mod, resamples = folds_lasso ,  grid = lasso_grid)

lambda_final <-
  select_best(lasso_res, metric = "rmse", maximize = FALSE) %>% select(penalty) %>% unlist

autoplot(lasso_res)

#apply best tuning parameter linear model ridge
lm_lasso_model <- linear_reg(penalty =  lambda_final, mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet") %>%
  fit(log_sale_price ~. , data = house_train)

#Visualise Feature Importance
vi(lm_lasso_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)),
                                  abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>%
  ggplot(aes(x = Variable, y = abs_importance, fill = Sign)) + geom_col() + coord_flip()

vi(lm_lasso_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)),
                                  abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>% ggplot(aes(x = Variable, y = Importance, fill = Sign)) + geom_col() + coord_flip()


#Visulaise beta strength
as.data.frame(as.matrix(lm_lasso_model$fit$beta)) %>%  rownames_to_column() %>% pivot_longer(-rowname) %>% group_by(rowname) %>% summarise(mean_beta = mean(value), se_beta = sd(value)) %>% ungroup %>% ggplot(aes(
  y = 1,
  x = mean_beta,
  alpha = 0.2,
  size = abs(mean_beta)
)) + geom_text(aes(label = rowname), position = position_jitter())


```

# evaluate and consolidate model metrics

```{r}

(
  map2_df(
    list(
      lm = list(lm = lm_model,name = "lm"),
      rf = list(rf = rf_model, name = "rf"),
      xg = list(xg = xg_model, name ="xg"),
      ridge = list(lm_ridge = lm_ridge_model, name = "ridge"),
      lasso = list(lm_lasso = lm_lasso_model, name = "lasso")
    ),
    list(
      house_train,
      house_train_treebased,
      house_train_treebased,
      house_train,
      house_train
    ),
    get_model_metrics
  ) %>% mutate(data = "train")
) %>%
  
  bind_rows(
    
    map2_df(
      list(
      lm = list(lm = lm_model,name = "lm"),
      rf = list(rf = rf_model, name = "rf"),
      xg = list(xg = xg_model, name ="xg"),
      ridge = list(lm_ridge = lm_ridge_model, name = "ridge"),
      lasso = list(lm_lasso = lm_lasso_model, name = "lasso")
    ),
      list(house_test, house_test_treebased, house_test_treebased, house_test, house_test),
      get_model_metrics
    )  %>% mutate(data = "test")
  ) %>%
  # pivot_wider(names_from = data, values_from = .estimate) %>%
  # unnest %>%
  
  #plot the metrics
  ggplot(aes(y = .estimate, x = m_type, color = data)) + geom_point(size = 4) + theme_light()
  
  
```


The lasso model gives the best test RMSE value. 

 








