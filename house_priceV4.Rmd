---
title: "Ames House Price prediction using tidymodel tools"
author: "Ravi Hela"
date: "22/03/2020"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

## Load libraries

```{r, message = FALSE, warning = FALSE}
library(tidymodels)
library(readr)
library(tidyverse)
library(lubridate)
library(skimr)
library(DataExplorer)
library(tidyquant)
library(corrr)
library(caret)
library(vip)
library(Hmisc)

```


## Define some useful functions

Caliing few function to help get correlation dataframe 


```{r,  message = FALSE, warning = FALSE}
#function to help filter corelated variables
source("correlation_df.R")
#get model metrics in a dataframe
source("model_metric_compare.R")
```


## Read data
```{r, message = FALSE, warning = FALSE}

train <- read_csv("train.csv")
test <- read_csv("test.csv")

names(train) <- make.names(names(train))
names(test) <- make.names(names(test))

```


The "test" file is the one to be used for Kaggle submission. This will not beused for model training but is a good for EDA use. 
For trainiing model we crate a validation set from train file later on. For model training we call the validation set as testing set

## Combinie test and train for EDA

```{r, message = FALSE, warning = FALSE, echo= FALSE}

#lapply(list(train = train, test = test), skim)
test$SalePrice <- NA
test$set <- "test"
train$set <- "train"
house_comb <- bind_rows(train, test)

```

Lets look at the summary of this combined dataset

```{r,  warning = FALSE, echo = FALSE}

skim(house_comb)

```


## EDA

### Lets do EDA for data quality.
We first check for missing values.

```{r, warning = FALSE, echo = FALSE}

profile_missing(house_comb) %>%
  arrange(desc(pct_missing))

plot_missing(house_comb)


```

Most of the higher missing values is result of absence of a particular feature in the property as per Data description. We will code the missing values with appropriate categorical variable later on. Rest of the features have very low missing percentages which can be handled during model data preprocessing using median or knnimpte.

#### Further Data Exploration

```{r, echo = FALSE, warning =  FALSE}
#check distribution of repsonse variabes

house_comb %>% 
  filter(set == "train") %>%
  mutate(log_sale_price = log10(SalePrice)) %>%
  select(SalePrice, log_sale_price) %>%
  pivot_longer(everything(), names_to = "Sale", values_to = "price") %>%
  ggplot(aes(x = price)) +
  geom_histogram() + facet_wrap(Sale~., scales = "free") +
  labs(title = "distribution of Price") +
  theme_bw()

```

Sale Price distribtion is right skewed however log transformation is Normal and is a candidate for Linear Regression.
Lets explore other features.


#### Check distribution of categorical variables

```{r, echo = FALSE, warning =  FALSE}
plot_bar(house_comb, ggtheme = theme_minimal())

```

#### Check distribution of Numeric features

```{r, echo = FALSE, warning =  FALSE}

plot_histogram(house_comb, ggtheme = theme_minimal())

```


#### Explore categorical variables against Sale price.
```{r, echo = FALSE, warning =  FALSE}

house_comb %>% mutate(SalePrice = SalePrice/1000) %>%
plot_boxplot(by = "SalePrice") 

```


#### Checking correlation among variables

```{r, echo = FALSE, warning =  FALSE}

house_comb %>%
  select_if(is.numeric) %>%
  corrr::correlate() %>%
  rearrange() %>%
  shave() %>% rplot + theme(axis.text.x = element_text(angle = 90))

house_comb %>% get_corr_df_simple(cut_off = 0.6)

```


#### Check for near zero variables

```{r, echo = FALSE, warning =  FALSE}

house_comb[, nearZeroVar(house_comb)] %>%
  mutate_if(is.character, as.factor) %>%
  summary

```


### Curious to know if street type makes a difference
```{r, echo = FALSE, warning =  FALSE}

house_comb %>% filter(set == "train") %>%
  group_by(Street) %>%
  summarise(sumry = list(summary(SalePrice))) %>% 
  mutate(sumry = map(sumry, ~ data.frame(t(.)))) %>%
  unnest(sumry) %>% pivot_wider(names_from = Street, values_from = Freq)

```

In general we do see House in pavement have higher Sale price than the Gravel.



### Checking affect of time over Sale Price

```{r, echo = FALSE, warning =  FALSE}

house_comb %>% 
  filter(set == "train") %>%
  select_at(vars(starts_with("Year"), starts_with("MoSold"), SalePrice)) %>%
  pivot_longer(-SalePrice, names_to = "time_metric", values_to = "unit") %>%
  ggplot(aes(y = SalePrice, x = unit )) + geom_point() + facet_wrap(.~time_metric, scales = "free")
  


house_comb %>% 
  filter(set == "train") %>%
  select_at(vars(starts_with("Year"), starts_with("MoSold"), SalePrice)) %>%
  pivot_longer(-SalePrice, names_to = "time_metric", values_to = "unit") %>%
  ggplot(aes(y = SalePrice, x = as.factor(unit) )) + geom_boxplot() + facet_wrap(.~time_metric, scales = "free") + theme_light()
  

```


There is no particular effect of month sold on Sale price. However, In general we dprices does increase Year on Year which makes common sense.


# Data Cleaning Based on our EDA

The code below will fix few missing values which are actually features not present in a property.
These features are related to
. Alley
. Bsmt
. Garage
. FireplaceQu
. PoolQC
. Fence
. MiscFeatures
. MasVnr

We also create a categorical bucketing variable out of YearSold and Yr Modelled. We create new variabe 'log_sale_price' that is log of Sale Price. This new variable will be used as  response variables.

Last we remove Id, Sale Price and set variable that will not contribute to the model.

```{r, message = FALSE, warning =  FALSE}

#pp3
house <- train %>%
  
  #converting sales to log scale
  mutate(log_sale_price = log(SalePrice)) %>%
  
  
  #fill in missing values NA for factor variables as per data description
  
  #Alley
  mutate(Alley = if_else(is.na(Alley), "No Alley", Alley)) %>%
  
  #Bsmt
  mutate(
    BsmtCond = if_else(is.na(BsmtCond), "No Bsmnt", BsmtCond),
    BsmtExposure =  if_else(is.na(BsmtExposure), "No Bsmnt", BsmtExposure),
    BsmtQual = if_else(is.na(BsmtQual), "No Bsmnt", BsmtQual),
    BsmtFinType1 =  if_else(is.na(BsmtFinType1), "No Bsmnt", BsmtFinType1),
    BsmtFinType2 =  if_else(is.na(BsmtFinType2), "No Bsmnt", BsmtFinType2),
    BsmtFinSF1 =  if_else(is.na(BsmtFinSF1), 0, BsmtFinSF1),
    BsmtFinSF2 =  if_else(is.na(BsmtFinSF2), 0, BsmtFinSF2),
    BsmtUnfSF =  if_else(is.na(BsmtUnfSF), 0, BsmtUnfSF),
    TotalBsmtSF =  if_else(is.na(TotalBsmtSF), 0, TotalBsmtSF),
    BsmtFullBath =  if_else(is.na(BsmtFullBath), 0, BsmtFullBath),
    BsmtHalfBath =  if_else(is.na(BsmtHalfBath), 0, BsmtHalfBath)
    
  ) %>%
  
  #Garage
  #GarageType, GarageType, GarageFinish, GarageQual, GarageCond, 'GarageYrBlt', 'GarageArea', 'GarageCars'
  mutate(
    GarageType = if_else(is.na(GarageType), "No Garage", GarageType),
    GarageFinish =  if_else(is.na(GarageFinish), "No Garage", GarageFinish),
    GarageQual = if_else(is.na(GarageQual), "No Garage", GarageQual),
    GarageCond =  if_else(is.na(GarageCond), "No Garage", GarageCond),
    GarageYrBlt =  if_else(is.na(GarageYrBlt), 0, GarageYrBlt),
    GarageArea =  if_else(is.na(GarageArea), 0, GarageArea),
    GarageCars =  if_else(is.na(GarageCond), 0, GarageCars)
  ) %>%
  
  mutate(MasVnrType = if_else(is.na(MasVnrType), "No MasVnrType", MasVnrType)) %>%
  mutate(MasVnrArea = if_else(is.na(MasVnrArea), 0, MasVnrArea)) %>%
  
  
  #FireplaceQu, PoolQC,Fence, MiscFeature
  mutate(
    FireplaceQu = if_else(is.na(FireplaceQu), "No Fireplc", FireplaceQu),
    PoolQC =  if_else(is.na(PoolQC), "No Pool", PoolQC),
    Fence = if_else(is.na(Fence), "No Fence", Fence),
    MiscFeature =  if_else(is.na(MiscFeature), "None", MiscFeature)
  ) %>%
  
  mutate(YrBuilt_cat = cut2(YearBuilt, cuts = seq(min(YearBuilt), max(YearBuilt), 5))) %>%
  
  mutate(YearRemodAdd = if_else(is.na(YearRemodAdd), YearBuilt, YearRemodAdd)) %>%
  
  
  #remove non contributing features and IDs
  select(-Id,-SalePrice, -set)
```



# Creating a Training and Validation(test) set using rsample

```{r, message = FALSE, warning =  FALSE}

set.seed(1234)
house_split <- initial_split(house)

train_split <- training(house_split)

```


# Create recepies with series of preprocessing steps on training set

```{r, message = FALSE, warning =  FALSE}

house_recipe <- train_split %>%
  recipe(log_sale_price ~ .) %>%
  step_string2factor(all_nominal(), -all_outcomes()) %>%
  step_num2factor(MoSold, levels = as.character(unique(train_split$MoSold))) %>%
  step_num2factor(YrSold, levels = as.character(unique(train_split$YrSold))) %>%
  step_num2factor(MSSubClass, levels = as.character(unique(train_split$MSSubClass))) %>%
  step_num2factor(OverallCond, levels = as.character(unique(train_split$OverallCond))) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  step_corr(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_knnimpute(all_predictors(), -all_outcomes()) 
  
  
doParallel::registerDoParallel()
house_prep <- prep(house_recipe)


house_recipe_treebased <- train_split %>%
  recipe(log_sale_price ~ .) %>%
  step_string2factor(all_nominal(), -all_outcomes()) %>%
  step_num2factor(MoSold, levels = as.character(unique(train_split$MoSold))) %>%
  step_num2factor(YrSold, levels = as.character(unique(train_split$YrSold))) %>%
  step_num2factor(OverallCond, levels = as.character(unique(train_split$OverallCond))) %>%
  step_num2factor(MSSubClass, levels = as.character(unique(train_split$MSSubClass))) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_corr(all_numeric()) %>%
  step_nzv(all_numeric()) %>%
  step_knnimpute(all_predictors(), -all_outcomes())

doParallel::registerDoParallel()    
house_treebased_prep <- prep(house_recipe_treebased)

```


# Applying preprocessing on training and validation(testing) set


```{r, message = FALSE, warning =  FALSE}

house_train <- juice(house_prep)
house_test <- bake(house_prep, testing(house_split))

#splits for tree based models
house_train_treebased <- juice(house_treebased_prep)
house_test_treebased <- bake(house_treebased_prep, testing(house_split))

```



## Train models

### Train linear regression


```{r, message = FALSE, warning =  FALSE}

#linear model
lm_model <- #recipe(log_sale_price ~ . , data = house_train) %>%
  linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm") %>%
  fit(log_sale_price ~ . , data = house_train)


```


Important variables from linear model

```{r, warning =  FALSE, echo = FALSE}

summary(lm_model$fit)$coefficients[, c(1, 4)] %>% data.frame() %>% rownames_to_column(var = "variables") %>% filter(Pr...t.. < .05, variables != "(Intercept)") %>% top_n(20, abs(Estimate)) %>% mutate(variables = as.factor(fct_reorder(variables, abs(Estimate)))) %>% ggplot(aes(x = variables, y = Estimate)) + geom_col() + coord_flip()

```

### Train random forest
```{r, message = FALSE, warning =  FALSE}

#create recepie on the preped house train data
rf_rec <-
  recipe(log_sale_price ~. , data = house_train_treebased)

#give model spec
rf_mod <-
  rand_forest(mtry = tune(), min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("regression")

#create Search grid
rf_grid <-
  grid_regular(mtry(range = c(15,40)), min_n(range= c(10, 2)), levels = 5)

#create samples for cross validation
folds <- vfold_cv(house_train_treebased, v = 10)


doParallel::registerDoParallel()

#create models with grid search
rf_res <-
  tune_grid(model = rf_mod, rf_rec,resamples = folds ,  grid = rf_grid)

final_mtry <- select_best(rf_res, "rmse", maximize = FALSE)$mtry
final_min_node <- select_best(rf_res, "rmse", maximize = FALSE)$min_n

#random forest
rf_model <- rand_forest(mtry = final_mtry, min_n = final_min_node) %>%
  set_mode("regression") %>%
  set_engine("ranger",importance = 'impurity') %>%
  fit(log_sale_price ~ . , data = house_train_treebased)

```


Important variables from randome forest model

```{r, echo = FALSE, warning =  FALSE}

#Visualise Feature Importance
vi(rf_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)),
                            abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>%
  ggplot(aes(x = Variable, y = abs_importance)) + geom_col() + coord_flip()

```


### train xgboost
```{r, message = FALSE, warning =  FALSE}

doParallel::registerDoParallel()
#xgboost
xg_model <- boost_tree() %>%
  set_mode("regression") %>%
  set_engine("xgboost") %>%
  fit(log_sale_price ~ . , data = house_train_treebased)

```


Important variables from xgoost model

```{r,  warning =  FALSE, echo = FALSE}

vi(xg_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)), abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>%
ggplot(aes(x = Variable, y = abs_importance)) + geom_col() + coord_flip()

```

### train linear model ridge
```{r,  warning =  FALSE, message = FALSE}

ridge_rec <-
  recipe(log_sale_price ~. , data = house_train)

ridge_mod <-
  linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")
 

ridge_grid <- expand.grid(penalty = seq(from = 0.01, to = 0.2, by=0.001), mixture = 0)

folds <- vfold_cv(house_train, v = 10)

doParallel::registerDoParallel()

ridge_res <-
  tune_grid(ridge_rec, model = ridge_mod, resamples = folds ,  grid = ridge_grid)

autoplot(ridge_res)

final_penalty <- select_best(ridge_res, "rmse", maximize = FALSE)$penalty

#apply best tuning parameter linear model ridge
lm_ridge_model <- linear_reg(penalty = final_penalty , mixture = 0) %>%
  set_mode("regression") %>%
  set_engine("glmnet") %>%
  fit(log_sale_price ~. , data = house_train)

```



Important variables from ridge model

```{r,  warning =  FALSE, echo = FALSE}

#Visualise Feature Importance
vi(lm_ridge_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)), abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>%
ggplot(aes(x = Variable, y = abs_importance, fill = Sign)) + geom_col() + coord_flip()

vi(lm_ridge_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)), abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>% ggplot(aes(x = Variable, y = Importance, fill = Sign)) + geom_col() + coord_flip()

#Visulaise beta strength
as.data.frame(as.matrix(lm_ridge_model$fit$beta)) %>%  rownames_to_column() %>% pivot_longer(-rowname) %>% group_by(rowname) %>% summarise(mean_beta = mean(value), se_beta = sd(value)) %>% ungroup %>% ggplot(aes(y = 1, x = mean_beta, , size = abs(mean_beta)), alpha = 0.2) + geom_text(aes(label = rowname), position = position_jitter())


```



### linear model lasso
```{r,  warning =  FALSE, message = FALSE}

lasso_rec <-
  recipe(log_sale_price ~. , data = house_train)

lasso_mod <-
  linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")
 
lasso_grid <- expand.grid(penalty = seq(from = 0.001, to = 0.005, by = 0.000001), mixture = 1)

folds_lasso <- vfold_cv(house_train, v = 10)

lasso_res <-
  tune_grid(lasso_rec, model = lasso_mod, resamples = folds_lasso ,  grid = lasso_grid)

lambda_final <-
  select_best(lasso_res, metric = "rmse", maximize = FALSE) %>% select(penalty) %>% unlist

autoplot(lasso_res)

#apply best tuning parameter linear model ridge
lm_lasso_model <- linear_reg(penalty =  lambda_final, mixture = 1) %>%
  set_mode("regression") %>%
  set_engine("glmnet") %>%
  fit(log_sale_price ~. , data = house_train)
```


Important variables from lasso model

```{r,  warning =  FALSE, echo = FALSE}

#Visualise Feature Importance
vi(lm_lasso_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)),
                                  abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>%
  ggplot(aes(x = Variable, y = abs_importance, fill = Sign)) + geom_col() + coord_flip()

vi(lm_lasso_model$fit) %>% mutate(Variable = fct_reorder(Variable, abs(Importance)),
                                  abs_importance = abs(Importance)) %>% arrange(desc(abs_importance)) %>% top_n(20) %>% ggplot(aes(x = Variable, y = Importance, fill = Sign)) + geom_col() + coord_flip()


#Visulaise beta strength
as.data.frame(as.matrix(lm_lasso_model$fit$beta)) %>%  rownames_to_column() %>% pivot_longer(-rowname) %>% group_by(rowname) %>% summarise(mean_beta = mean(value), se_beta = sd(value)) %>% ungroup %>% ggplot(aes(
  y = 1,
  x = mean_beta,
  alpha = 0.2,
  size = abs(mean_beta)
)) + geom_text(aes(label = rowname), position = position_jitter())


```

# Evaluate and consolidate model metrics

```{r,  warning =  FALSE, echo = FALSE}

model_evaluation <- (
  map2_df(
    list(
      lm = list(lm = lm_model,name = "lm"),
      rf = list(rf = rf_model, name = "rf"),
      xg = list(xg = xg_model, name ="xg"),
      ridge = list(lm_ridge = lm_ridge_model, name = "ridge"),
      lasso = list(lm_lasso = lm_lasso_model, name = "lasso")
    ),
    list(
      house_train,
      house_train_treebased,
      house_train_treebased,
      house_train,
      house_train
    ),
    get_model_metrics
  ) %>% mutate(data = "train")
) %>%
  
  bind_rows(
    
    map2_df(
      list(
      lm = list(lm = lm_model,name = "lm"),
      rf = list(rf = rf_model, name = "rf"),
      xg = list(xg = xg_model, name ="xg"),
      ridge = list(lm_ridge = lm_ridge_model, name = "ridge"),
      lasso = list(lm_lasso = lm_lasso_model, name = "lasso")
    ),
      list(house_test, house_test_treebased, house_test_treebased, house_test, house_test),
      get_model_metrics
    )  %>% mutate(data = "test")
  ) 
  

model_evaluation %>%  pivot_wider(names_from = data, values_from = .estimate)

  
#plot the metrics
model_evaluation %>% ggplot(aes(y = .estimate, x = m_type, color = data)) + geom_point(size = 4) + theme_light()
  
  
```


# Select Model
The lasso model gives the best test RMSE value. 

 








